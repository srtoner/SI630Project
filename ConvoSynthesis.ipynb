{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import TwitterUtils as TU\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# import spacy\n",
    "# import spacy_langdetect as sld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tweepy\n",
    "import time\n",
    "\n",
    "with open('my_oauth.json', 'r') as f:\n",
    "    oauth_tokens = json.load(f)\n",
    "\n",
    "\n",
    "consumer_key = os.environ.get(\"CONSUMER_KEY\")\n",
    "consumer_secret = os.environ.get(\"CONSUMER_SECRET\")\n",
    "access_token = oauth_tokens['oauth_token']\n",
    "access_token_secret = oauth_tokens['oauth_token_secret']\n",
    "\n",
    "# tweepy workflow, not sure how it differs from twint\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Data\n",
    "\n",
    "The `GetPlaces.py`, `GetTweets.py`, and `SampleUser.py` files have generated the following output files:\n",
    "* users.json : contains all the user specific data from the 51,000 sampled users\n",
    "* places.pkl : metadata related to all twitter places in the user sample\n",
    "* tweets.pkl : retrieved 100 tweets from each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_regex = 'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&\\/\\/=]*)'\n",
    "twitter_username_re = '(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)'\n",
    "twitter_username_re = r\"((^|[^@\\w])@(\\w{1,15})\\b)*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"places.pkl\", \"rb\") as file: # Unique Places\n",
    "    places = pkl.load(file)\n",
    "\n",
    "with open('tweets.pkl', 'rb') as file: # Rename to Tweets, user_id, place_id\n",
    "    data = pkl.load(file)\n",
    "\n",
    "with open('users.json', 'r') as file: # Actual data for user accounts\n",
    "    user_json = file.read()\n",
    "\n",
    "test = '{\"total\": [' + user_json.replace(\"}{\", \"},{\") + \"]}\"\n",
    "user_data = json.loads(test)\n",
    "users = [u['data'] for u in user_data[\"total\"]]\n",
    "flat_list = [user_id for user in users for user_id in user]\n",
    "\n",
    "with open(\"user_data_convo.pkl\", \"rb\") as file:\n",
    "    convos = pkl.load(file)\n",
    "\n",
    "with open(\"timestamps.pkl\", \"rb\") as file:\n",
    "    timestamps = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_data = [t['data'] for t in timestamps]\n",
    "ts_list = []\n",
    "\n",
    "for t in timestamp_data:\n",
    "    ts_list = ts_list + [*t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_unpacked = [item  for item in places.values()]\n",
    "def unpack_place(place):\n",
    "    return (place.id, place.name, place.full_name, place.country, place.country_code, place.place_type, place.centroid[0], place.centroid[1])\n",
    "\n",
    "unpacked_places = [unpack_place(place) for place in places_unpacked]\n",
    "place_df = pd.DataFrame(unpacked_places, columns = (\"id\", \"name\", \"full_name\", \"country\", \"country_code\", \"type\", \"lat\", \"lon\"))\n",
    "users_df = pd.DataFrame(flat_list).rename(columns = {'name' : 'user_name_field'})\n",
    "convo_df = pd.DataFrame(convos)\n",
    "ts_df = pd.DataFrame(ts_list) # For merging with convo dataset\n",
    "data_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_dict = {}\n",
    "\n",
    "for tsl in ts_list:\n",
    "    if convo_dict.get(tsl['conversation_id']):\n",
    "        convo_dict[tsl['conversation_id']].append(tsl)\n",
    "    else:\n",
    "        convo_dict[tsl['conversation_id']] = [tsl]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = [(cd['id'], cd['author_id'], cd['text']) for cd in convo_dict[entry]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1636437653808902150',\n",
       "  '1051074201082388480',\n",
       "  'This is the remedy which will ensure wealth and prosperity to you and your family. ðŸ™ https://t.co/J8Gn2mQUkv'),\n",
       " ('1636438018067423232',\n",
       "  '1518203416752783360',\n",
       "  '@Pavanasoonu No idea sir ..how can I find out?'),\n",
       " ('1636570176278917121',\n",
       "  '949964790',\n",
       "  '@Pavanasoonu How can I find this guru ji\\n\\nIk my gotra only'),\n",
       " ('1636570938300047361',\n",
       "  '1051074201082388480',\n",
       "  '@vyaspranjal33 Ask the elders in the family. #AskPanditKatti'),\n",
       " ('1636592187319234561',\n",
       "  '1590604811321630720',\n",
       "  '@Pavanasoonu Is it possible to find pravara if you know your Gothra?')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in convo_dict:\n",
    "    convo_dict[entry].sort(key=lambda x: x['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = convo_df.merge(ts_df[['id', 'created_at']], how = 'left', left_on = 'tweet_id', right_on = 'id')\n",
    "test = merged.merge(users_df, how = 'left', left_on = 'user_id', right_on ='id' )\n",
    "test = test.drop(columns = ['id_x', 'id_y'])\n",
    "test2 = test.merge(data_df[['tweet_id', 'place_id']], how = 'left', left_on = 'tweet_id', right_on = 'tweet_id')\n",
    "with_places = test2.merge(place_df, how = 'left', left_on = 'place_id', right_on = 'id')\n",
    "with_places_list = with_places.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '1013303513449836547',\n",
       " 'tweet_id': '1638630321473335297',\n",
       " 'tweet_text': '@1shankarsharma @Pavanasoonu Think.... astrologers are more accurate than market pundits.',\n",
       " 'referenced_tweets': [{'type': 'replied_to', 'id': '1637748357127864320'}],\n",
       " 'convo_id': '1637733079002537986',\n",
       " 'reply_to_user_id': '2989553946',\n",
       " 'created_at': '2023-03-22T19:54:47.000Z',\n",
       " 'username': nan,\n",
       " 'description': nan,\n",
       " 'user_name_field': nan,\n",
       " 'location': nan,\n",
       " 'withheld': nan,\n",
       " 'place_id': nan,\n",
       " 'id': nan,\n",
       " 'name': nan,\n",
       " 'full_name': nan,\n",
       " 'country': nan,\n",
       " 'country_code': nan,\n",
       " 'type': nan,\n",
       " 'lat': nan,\n",
       " 'lon': nan}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_places_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_dict_location = {}\n",
    "\n",
    "for wpl in with_places_list:\n",
    "    if convo_dict_location.get(wpl['convo_id']):\n",
    "        convo_dict_location[wpl['convo_id']]['tweets'].append(wpl)\n",
    "    else:\n",
    "        convo_dict_location[wpl['convo_id']] = {}\n",
    "        convo_dict_location[wpl['convo_id']]['tweets'] = [wpl]\n",
    "    if not pd.isna(wpl['place_id']):\n",
    "        convo_dict_location[wpl['convo_id']]['place_id'] = wpl['place_id']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in convo_dict_location:\n",
    "    convo_dict_location[entry]['tweets'] = pd.DataFrame(convo_dict_location[entry]['tweets']).drop_duplicates(subset = 'tweet_id').to_dict('records')\n",
    "    convo_dict_location[entry]['tweets'].sort(key=lambda x: x['tweet_id'])\n",
    "    joined_tweets = []\n",
    "    user_ids = []\n",
    "    last_id = \"Blank\"\n",
    "    for tweet in convo_dict_location[entry]['tweets']:\n",
    "        if last_id == tweet['user_id']:\n",
    "            user_string = \" \"\n",
    "            joined_tweets[-1] = joined_tweets[-1][:-2] # Remove delimiter for new person talking\n",
    "        else:\n",
    "            user_string = '[' + tweet['user_id'] + '] ' \n",
    "            last_id = tweet['user_id'] \n",
    "\n",
    "        joined_tweets.append(user_string + tweet['tweet_text'] + '||')\n",
    "        user_ids.append(tweet['user_id'])\n",
    "    convo_dict_location[entry]['tweet_list'] = joined_tweets\n",
    "    convo_dict_location[entry]['user_list'] = list(set(user_ids))\n",
    "    convo_dict_location[entry]['tweet_joined'] = \" \".join(joined_tweets)\n",
    "    for user in convo_dict_location[entry]['user_list']:\n",
    "        # Rather than having user_ids in the prompt themselves, we want to have numbers representing the participants in the dialogue\n",
    "        convo_dict_location[entry]['tweet_joined'] = convo_dict_location[entry]['tweet_joined'].replace(user, str(convo_dict_location[entry]['user_list'].index(user)))\n",
    "    if not convo_dict_location[entry].get('place_id'):\n",
    "        convo_dict_location[entry]['place_id'] = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_extract = [{'convo_id': entry, 'user_list' : cdl['user_list'], 'tweet_list': cdl['tweet_list'], 'tweet_joined': cdl['tweet_joined'], 'place_id': cdl['place_id']} for entry, cdl in convo_dict_location.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_extract_df = pd.DataFrame(convo_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_extract_df = convo_extract_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_places = convo_extract_df.merge(place_df, how = 'left', left_on = 'place_id', right_on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[0] @Har1AUM Here are a few of my recommendations.  Jaimini upadesa sutras and BPHS.  Prasna marga is extremely detailed on badhaka, sarpa issues.|| [2] @Pavanasoonu Badhaka I know is a topic discussed in Prasna Marga. BPHS and such voluminous texts have innumerable precepts. Practical application I am looking for. Every old time precept may not be valid today. How many families give due respect for Kuladevatas? Who is immune to their wrath?|| [0] @Har1AUM This needs decades of practical application.  Indeed Kala Patra and sandharbha is always needed.  Verbatim application of any shloka is not advisable.  This is taken up during the chart discussion.  #askpanditkatti|| [2] @Pavanasoonu Do you have even one example that can be shared? Kuladevatas is a very important topic. Lineage has to be reckoned as maternal or paternal? It's different for different communities. Practical application is crucial to understand such things.|| [0] @Har1AUM Namaste.  I do have numerous examples I have avdiced appropriately in my consultations.  I have taught a master class on this with real-time charts. \\n\\nGoing into these in detail in SM is not possible.  If time permits I will plan for a zoom session.|| [2] @Pavanasoonu Maternal or paternal lineage reckoned? Martial tribes like Bhils, to my knowledge had the maternal lineage Ta-vazhi import. Choosing the wrong lineage may upset the whole analysis. Sakyas, Yadavas etc were of maternal lineage originally.|| [0] @Har1AUM Yes.  The sampradaya understanding of one's own is Important. #askpanditkatti|| [1] @Pavanasoonu Namaste Guruji, what is AK and Bhadakesh are the same graha? 12th from both overlap, in such cases how does one evaluate relationship with deity and wrath?\\n\\nPranamam ðŸ™|| [3] @Rolfgolfball @Pavanasoonu One is seen in Rashi (Badhakesha) the other is seen in D9 (Ishta).|| [1] @gajanansinamdar @Pavanasoonu Thank you Gajanan ji.||\""
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_places.iloc[5].tweet_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>convo_id</th>\n",
       "      <th>user_list</th>\n",
       "      <th>tweet_list</th>\n",
       "      <th>tweet_joined</th>\n",
       "      <th>place_id</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>country_code</th>\n",
       "      <th>type</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>United States</th>\n",
       "      <td>7400</td>\n",
       "      <td>7400</td>\n",
       "      <td>7400</td>\n",
       "      <td>7400</td>\n",
       "      <td>7400</td>\n",
       "      <td>7400</td>\n",
       "      <td>7400</td>\n",
       "      <td>7400</td>\n",
       "      <td>7400</td>\n",
       "      <td>7400</td>\n",
       "      <td>7400</td>\n",
       "      <td>7400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom</th>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Canada</th>\n",
       "      <td>679</td>\n",
       "      <td>679</td>\n",
       "      <td>679</td>\n",
       "      <td>679</td>\n",
       "      <td>679</td>\n",
       "      <td>679</td>\n",
       "      <td>679</td>\n",
       "      <td>679</td>\n",
       "      <td>679</td>\n",
       "      <td>679</td>\n",
       "      <td>679</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>India</th>\n",
       "      <td>476</td>\n",
       "      <td>476</td>\n",
       "      <td>476</td>\n",
       "      <td>476</td>\n",
       "      <td>476</td>\n",
       "      <td>476</td>\n",
       "      <td>476</td>\n",
       "      <td>476</td>\n",
       "      <td>476</td>\n",
       "      <td>476</td>\n",
       "      <td>476</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nigeria</th>\n",
       "      <td>438</td>\n",
       "      <td>438</td>\n",
       "      <td>438</td>\n",
       "      <td>438</td>\n",
       "      <td>438</td>\n",
       "      <td>438</td>\n",
       "      <td>438</td>\n",
       "      <td>438</td>\n",
       "      <td>438</td>\n",
       "      <td>438</td>\n",
       "      <td>438</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Australia</th>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ireland</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mexico</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Belgium</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Denmark</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kenya</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Zealand</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Arab Emirates</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      convo_id  user_list  tweet_list  tweet_joined  place_id  \\\n",
       "country                                                                         \n",
       "United States             7400       7400        7400          7400      7400   \n",
       "United Kingdom            2003       2003        2003          2003      2003   \n",
       "Canada                     679        679         679           679       679   \n",
       "India                      476        476         476           476       476   \n",
       "Nigeria                    438        438         438           438       438   \n",
       "Australia                  183        183         183           183       183   \n",
       "Ireland                      3          3           3             3         3   \n",
       "Germany                      2          2           2             2         2   \n",
       "Mexico                       2          2           2             2         2   \n",
       "Belgium                      1          1           1             1         1   \n",
       "Denmark                      1          1           1             1         1   \n",
       "Kenya                        1          1           1             1         1   \n",
       "New Zealand                  1          1           1             1         1   \n",
       "United Arab Emirates         1          1           1             1         1   \n",
       "\n",
       "                        id  name  full_name  country_code  type   lat   lon  \n",
       "country                                                                      \n",
       "United States         7400  7400       7400          7400  7400  7400  7400  \n",
       "United Kingdom        2003  2003       2003          2003  2003  2003  2003  \n",
       "Canada                 679   679        679           679   679   679   679  \n",
       "India                  476   476        476           476   476   476   476  \n",
       "Nigeria                438   438        438           438   438   438   438  \n",
       "Australia              183   183        183           183   183   183   183  \n",
       "Ireland                  3     3          3             3     3     3     3  \n",
       "Germany                  2     2          2             2     2     2     2  \n",
       "Mexico                   2     2          2             2     2     2     2  \n",
       "Belgium                  1     1          1             1     1     1     1  \n",
       "Denmark                  1     1          1             1     1     1     1  \n",
       "Kenya                    1     1          1             1     1     1     1  \n",
       "New Zealand              1     1          1             1     1     1     1  \n",
       "United Arab Emirates     1     1          1             1     1     1     1  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_places.groupby('country').agg('count').sort_values(by = 'convo_id', ascending=False).head(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Tweet Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Including my SAP technology business.  Thank you. Namaste.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(URL_regex, \"\", text_test)\n",
    "re.sub(twitter_username_re, \"\", text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convo_places['tweet_joined'] = convo_places.tweet_list.apply(''.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    temp = re.sub(URL_regex, \"\", text).strip()\n",
    "\n",
    "    return re.sub(twitter_username_re, \"\", text)\n",
    "\n",
    "convo_places['clean_convo'] = convo_places.tweet_joined.apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_places['thread_length'] = convo_places['tweet_list'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_places = convo_places[convo_places.thread_length >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convo_id         4177\n",
       "user_list        4177\n",
       "tweet_list       4177\n",
       "tweet_joined     4177\n",
       "place_id         4177\n",
       "id               4177\n",
       "full_name        4177\n",
       "country          4177\n",
       "country_code     4177\n",
       "type             4177\n",
       "lat              4177\n",
       "lon              4177\n",
       "clean_convo      4177\n",
       "thread_length    4177\n",
       "dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_places.groupby('name').agg('count').sort_values(by = 'convo_id', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_places.to_csv('convo_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
