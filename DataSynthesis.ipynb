{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import TwitterUtils as TU\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "import spacy_langdetect as sld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Data\n",
    "\n",
    "The `GetPlaces.py`, `GetTweets.py`, and `SampleUser.py` files have generated the following output files:\n",
    "* users.json : contains all the user specific data from the 51,000 sampled users\n",
    "* places.pkl : metadata related to all twitter places in the user sample\n",
    "* tweets.pkl : retrieved 100 tweets from each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_regex = 'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&\\/\\/=]*)'\n",
    "twitter_username_re = '(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)'\n",
    "twitter_username_re = r\"((^|[^@\\w])@(\\w{1,15})\\b)*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"places.pkl\", \"rb\") as file: # Unique Places\n",
    "    places = pkl.load(file)\n",
    "\n",
    "with open('tweets.pkl', 'rb') as file: # Rename to Tweets, user_id, place_id\n",
    "    data = pkl.load(file)\n",
    "\n",
    "with open('users.json', 'r') as file: # Actual data for user accounts\n",
    "    user_json = file.read()\n",
    "\n",
    "test = '{\"total\": [' + user_json.replace(\"}{\", \"},{\") + \"]}\"\n",
    "user_data = json.loads(test)\n",
    "users = [u['data'] for u in user_data[\"total\"]]\n",
    "flat_list = [user_id for user in users for user_id in user]\n",
    "\n",
    "users_df = pd.DataFrame(flat_list).rename(columns = {'name' : 'user_name_field'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_unpacked = [item  for item in places.values()]\n",
    "def unpack_place(place):\n",
    "    return (place.id, place.name, place.full_name, place.country, place.country_code, place.place_type)\n",
    "\n",
    "unpacked_places = [unpack_place(place) for place in places_unpacked]\n",
    "place_df = pd.DataFrame(unpacked_places, columns = (\"id\", \"name\", \"full_name\", \"country\", \"country_code\", \"type\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL_regex = 'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_regex = r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&\\/\\/=]*)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "merged = pd.merge(df, place_df, how = 'left', left_on = 'place_id', right_on='id')\n",
    "full_data = pd.merge(merged, users_df, how='left', left_on ='user_id', right_on = 'id')\n",
    "# full_data.to_csv('fulldata.csv', index = False) # 3 Gigs of data, not great...\n",
    "# full_data.description.iloc[0,]\n",
    "# places_unpacked # Cool opportunity for geographic visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30470"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_data['user_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9066735266648764"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit to top 6 countries\n",
    "by_country = merged.groupby('country').count()\n",
    "top6 = by_country.sort_values(by = 'user_id', ascending=False).head(6)\n",
    "total = by_country.user_id.sum()\n",
    "top6.user_id.divide(total).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'tweet_id', 'tweet_text', 'place_id', 'id_x', 'name',\n",
       "       'full_name', 'country', 'country_code', 'type', 'username',\n",
       "       'description', 'id_y', 'user_name_field', 'location', 'withheld'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 6 countries account for 91% of the total users collected, which suggests pretty good coverage. Dropping unnecessary columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fields = ['user_id', 'tweet_id', 'tweet_text', 'place_id', 'name',\n",
    "       'full_name', 'country', 'country_code', 'type', 'username',\n",
    "       'description', 'user_name_field', 'location']\n",
    "reduced_df = full_data[target_fields]\n",
    "reduced_df = reduced_df.rename(columns={'name':'place_name', \n",
    "                                        'full_name':'full_place_name',\n",
    "                                        'type': 'place_type', \n",
    "                                        'description':'profile_description',\n",
    "                                        'user_name_field':'profile_name',\n",
    "                                        'location':'profile_location'\n",
    "                                        })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "top6_countries = top6.index\n",
    "top6 = reduced_df[reduced_df['country'].isin(top6_countries)]\n",
    "unique_tweets = top6['tweet_id'].unique()\n",
    "top6 = top6.drop_duplicates('tweet_id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang_detector(nlp, name):\n",
    "    return sld.LanguageDetector()\n",
    "\n",
    "# Uncomment when running for first time\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy.Language.factory('language_detector', func = get_lang_detector)\n",
    "nlp.add_pipe('language_detector', last =True)\n",
    "\n",
    "def get_language(text):\n",
    "    return nlp(text)._.language['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999960942533268"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_language(top6.tweet_text.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = top6.to_dict(orient='records')\n",
    "langs = [get_language(d['tweet_text']) for d in data_dict]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Tweet Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Gajendr70729189 @amitsharma2704 @1shankarsharma Including my SAP technology business.  Thank you. Namaste.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test = top6.tweet_text.iloc[1]\n",
    "text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Including my SAP technology business.  Thank you. Namaste.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(URL_regex, \"\", text_test)\n",
    "re.sub(twitter_username_re, \"\", text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    temp = re.sub(URL_regex, \"\", text)\n",
    "\n",
    "    return re.sub(twitter_username_re, \"\", text)\n",
    "\n",
    "top6['clean_text'] = top6.tweet_text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(859183, 14)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top6 = top6.drop_duplicates('tweet_id')\n",
    "top6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "top6.to_csv('filtered_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
