{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import TwitterUtils as TU\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "import spacy_langdetect as sld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Data\n",
    "\n",
    "The `GetPlaces.py`, `GetTweets.py`, and `SampleUser.py` files have generated the following output files:\n",
    "* users.json : contains all the user specific data from the 51,000 sampled users\n",
    "* places.pkl : metadata related to all twitter places in the user sample\n",
    "* tweets.pkl : retrieved 100 tweets from each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_regex = 'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&\\/\\/=]*)'\n",
    "twitter_username_re = '(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)'\n",
    "twitter_username_re = r\"((^|[^@\\w])@(\\w{1,15})\\b)*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"places.pkl\", \"rb\") as file: # Unique Places\n",
    "    places = pkl.load(file)\n",
    "\n",
    "with open('tweets.pkl', 'rb') as file: # Rename to Tweets, user_id, place_id\n",
    "    data = pkl.load(file)\n",
    "\n",
    "with open('users.json', 'r') as file: # Actual data for user accounts\n",
    "    user_json = file.read()\n",
    "\n",
    "test = '{\"total\": [' + user_json.replace(\"}{\", \"},{\") + \"]}\"\n",
    "user_data = json.loads(test)\n",
    "users = [u['data'] for u in user_data[\"total\"]]\n",
    "flat_list = [user_id for user in users for user_id in user]\n",
    "\n",
    "users_df = pd.DataFrame(flat_list).rename(columns = {'name' : 'user_name_field'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_unpacked = [item  for item in places.values()]\n",
    "def unpack_place(place):\n",
    "    return (place.id, place.name, place.full_name, place.country, place.country_code, place.place_type)\n",
    "\n",
    "unpacked_places = [unpack_place(place) for place in places_unpacked]\n",
    "place_df = pd.DataFrame(unpacked_places, columns = (\"id\", \"name\", \"full_name\", \"country\", \"country_code\", \"type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Place' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mset\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mchain\u001b[39m.\u001b[39;49mfrom_iterable([\u001b[39m*\u001b[39;49mplace] \u001b[39mfor\u001b[39;49;00m place \u001b[39min\u001b[39;49;00m places_unpacked))\n",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mset\u001b[39m(itertools\u001b[39m.\u001b[39mchain\u001b[39m.\u001b[39mfrom_iterable([\u001b[39m*\u001b[39mplace] \u001b[39mfor\u001b[39;00m place \u001b[39min\u001b[39;00m places_unpacked))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Place' object is not iterable"
     ]
    }
   ],
   "source": [
    "set(itertools.chain.from_iterable([*place] for place in places_unpacked))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL_regex = 'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_regex = r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&\\/\\/=]*)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "merged = pd.merge(df, place_df, how = 'left', left_on = 'place_id', right_on='id')\n",
    "full_data = pd.merge(merged, users_df, how='left', left_on ='user_id', right_on = 'id')\n",
    "# full_data.to_csv('fulldata.csv', index = False) # 3 Gigs of data, not great...\n",
    "# full_data.description.iloc[0,]\n",
    "# places_unpacked # Cool opportunity for geographic visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30470"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_data['user_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9066735266648764"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit to top 6 countries\n",
    "by_country = merged.groupby('country').count()\n",
    "top6 = by_country.sort_values(by = 'user_id', ascending=False).head(6)\n",
    "total = by_country.user_id.sum()\n",
    "top6.user_id.divide(total).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'tweet_id', 'tweet_text', 'place_id', 'id_x', 'name',\n",
       "       'full_name', 'country', 'country_code', 'type', 'username',\n",
       "       'description', 'id_y', 'user_name_field', 'location', 'withheld'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 6 countries account for 91% of the total users collected, which suggests pretty good coverage. Dropping unnecessary columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fields = ['user_id', 'tweet_id', 'tweet_text', 'place_id', 'name',\n",
    "       'full_name', 'country', 'country_code', 'type', 'username',\n",
    "       'description', 'user_name_field', 'location']\n",
    "reduced_df = full_data[target_fields]\n",
    "reduced_df = reduced_df.rename(columns={'name':'place_name', \n",
    "                                        'full_name':'full_place_name',\n",
    "                                        'type': 'place_type', \n",
    "                                        'description':'profile_description',\n",
    "                                        'user_name_field':'profile_name',\n",
    "                                        'location':'profile_location'\n",
    "                                        })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "top6_countries = top6.index\n",
    "top6 = reduced_df[reduced_df['country'].isin(top6_countries)]\n",
    "unique_tweets = top6['tweet_id'].unique()\n",
    "top6 = top6.drop_duplicates('tweet_id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang_detector(nlp, name):\n",
    "    return sld.LanguageDetector()\n",
    "\n",
    "# Uncomment when running for first time\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy.Language.factory('language_detector', func = get_lang_detector)\n",
    "nlp.add_pipe('language_detector', last =True)\n",
    "\n",
    "def get_language(text):\n",
    "    return nlp(text)._.language['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = top6.to_dict(orient='records')\n",
    "langs = [get_language(d['tweet_text']) for d in data_dict]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Tweet Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Gajendr70729189 @amitsharma2704 @1shankarsharma Including my SAP technology business.  Thank you. Namaste.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test = top6.tweet_text.iloc[1]\n",
    "text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Including my SAP technology business.  Thank you. Namaste.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(URL_regex, \"\", text_test)\n",
    "re.sub(twitter_username_re, \"\", text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b1/vw0sn5w90nnccbfn7bskv0jm0000gn/T/ipykernel_2099/1597261052.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top6['clean_text'] = top6.tweet_text.apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    temp = re.sub(URL_regex, \"\", text)\n",
    "\n",
    "    return re.sub(twitter_username_re, \"\", text)\n",
    "\n",
    "top6['clean_text'] = top6.tweet_text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(859183, 15)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top6 = top6.drop_duplicates('tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "top6.to_csv('filtered_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
